

<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Object Detection Models</title>
    <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">
    <link rel="stylesheet" href="/main.css">
    <link rel="stylesheet" href="/syntax.css">
</head>
<body>
    <header>
        <div class="logo">
            <a href="/">desertBits</a>
        </div>
        <div class="links">
            <a href="https://github.com/lostVkng" target="_blank">Github</a>
            <a href="https://huggingface.co/lostvkng" target="_blank">HuggingFace</a>
        </div>
    </header>
    <main>
        
<article class="post">
    <header>
        <h1>Object Detection Models</h1>
        <time datetime="2025-02-13">February 13, 2025</time>
    </header>
    <div class="post-content">
        <ol>
<li><a href="#one-vs-two-stage-detection">One vs Two Stage Detection</a></li>
<li><a href="#shots-in-object-detection">Shots in Object Detection</a></li>
<li><a href="#popular-models">Popular Models</a></li>
<li><a href="#fine-tuning-a-yolo-model">Fine-Tuning a YOLO Model</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>

<p>Object detection models are a subset of computer vision models that seek to identify objects in an image. Using machine learning, detection models create bounding boxes with labels around each object. They differ from classification models in that they are looking for objects in the image rather than at the entire image. Object detection models are used across all industries, notable examples include self-driving vehicles and autonomous robots. Let&rsquo;s dive into characteristics of Object Detection models, a few popular models, and fine-tune our own model.</p>

<h3 id="one-vs-two-stage-detection">One vs Two Stage Detection</h3>

<p>Detection models can be grouped in two categories, One and Two Stage Detectors. The stage count refers to how many times an image is passed through a neural network. Two Stage detection was the original standard, first pioneered in 2014 with the release of R-CNN. It was quickly followed by One Stage detection in 2016.</p>

<p>The original <a href="https://arxiv.org/abs/1311.2524v5" target="_blank">R-CNN</a> paper was game changing for the computer vision field. It demonstrated that Convolutional Neural Networks (CNNs) could be used not only for classification but for identifying objects in an image through highlighted bounding boxes.</p>

<p>Two Stage Detection Architecture</p>

<ol>
<li>Extract Region Proposals</li>
<li>Feature Extraction from each region</li>
<li>Classification of Regions</li>
<li>Box Refinement</li>
<li>Non-Maximum Suppression (NMS) - Remove Overlapping boxes</li>
</ol>

<p><img src="/assets/od_rcnn.png" alt="RCNN Model" /></p>

<p>One major challenge with R-CNN and its iterations was the number of steps, models, and overall architecture to manage. Region extraction was executed with an algorithm separate of the CNN. The 2,000 regions are passed to the CNN to generate features. Then a classifier is used to classify each feature before refining the bounding boxes with a regression model. Finally the data is passed to another CNN to apply Non-maximum suppression (NMS) to remove any duplicates or overlapping boxes. Each step can and has been a bottleneck in different implementations resulting in a lot of time and effort to create a smooth processing pipeline.</p>

<h3 id="object-detection-shots">Object Detection Shots</h3>

<p>The number of &ldquo;shots&rdquo; for a model refers to how many training examples they require per class.</p>

<p><strong>Full Shot</strong> - Aka Standard supervised learning. Full Shot models are trained on a large fully labelled dataset. R-CNN and its later iterations such as Faster R-CNN fall into this boat. Full shot is generally considered to be the most accurate, but its training costs and slower inference can be cost prohibitive in the real world.</p>

<p><strong>Few Shot</strong> - The model is trained on a small subset of examples. These models are very useful when large sets of training data is not easy to acquire. Few Shot includes models like YOLO and RT-DETR.</p>

<p><strong>Zero Shot</strong> - The newest type of object detection model. Zero shot relies on generalized knowledge and has no training data for its detections. Often times requiring guidance or prompting from the user to identify objects without any previous examples.</p>

<h3 id="popular-models">Popular Models</h3>

<p><strong>You Only Look Once (YOLO)</strong>
Yolo models perform a single pass on an image, as opposed to earlier R-CNNs which requires thousands of passes per image. The YOLO model has gone through numerous iterations. Some of the recent versions can perform inference at north of 160fps making them great canidates for real-time applications where speed is critical.</p>

<p>Their simple architecture and fast performance have led to strong adoption in the market. While YOLO does come with its limitations such as difficulty detecting small objects.</p>

<p><a href="https://arxiv.org/abs/2106.00666" target="_blank">Initial Paper</a><br>
<a href="https://huggingface.co/Ultralytics/YOLOv8" target="_blank">YoloV8</a></p>

<p><img src="/assets/od_yolo.png" alt="Yolo Model" /></p>

<p><strong>Real-Time (RT) - End-to-End Object Detection (DETR)</strong>
RT-DETR was created to address the computational costs from YOLO models while serving realtime use cases. YOLO models utilize the same NMS technique above to remove overlapping boxes, this extra analysis can slow down inference or make it prohibitively expensive. RT-DETR leverages the Transformer model architecture to process input sequences in parallel. RT-DETR implements a hybrid encoder to decouple intra-scale interaction and  cross-scale fusion resulting in lowered computational costs and improved performance.</p>

<p><a href="https://arxiv.org/abs/2304.08069" target="_blank">Initial Paper</a><br>
<a href="https://huggingface.co/PekingU/rtdetr_v2_r101vd" target="_blank">RT-DETRv2</a></p>

<p><img src="/assets/od_detr.png" alt="DETR Model" /></p>

<p><strong>OWLv2</strong>
OWL is a zero shot object detection model. It combines user text prompts with its base knowledge to identify objects. This type of model is a new category because it requires no examples, only the context of the prompt to identify objects. OWL is very useful for use cases that may not have useful training data solving the rare item identification problem.</p>

<p><a href="https://arxiv.org/abs/2306.09683" target="_blank">Initial Paper</a><br>
<a href="https://huggingface.co/google/owlv2-base-patch16-ensemble" target="_blank">OWLv2-base-patch16-ensemble</a></p>

<p><img src="/assets/od_owl.png" alt="Owl Model" /></p>

<h3 id="fine-tuning-a-yolo-model">Fine-tuning a YOLO Model</h3>

<p>Fine-tuning is a technique that starts with a pre-trained model and adjusts it for a specific task. As an example, say I want to detect Lego Stormtroopers, the YOLO models are trained for ~80 classes, but not Stormtroopers. This is a great use case for fine-tuning. We will utilize the models existing knowledge but give it direction to the types of objects to identify. One of the benefits of fine-tuning is it typically requires less training data and is more performant as it effectively creates hot paths to specific objects.</p>

<p>First, we need to get some training data. I&rsquo;ve taken ~50 images of Stormtroopers on my table and annotated each of them. It is usually recommended to have 2k-10k images per class, but let&rsquo;s see how accurate we can get with 50! There are lots of annotation tools, I&rsquo;ve commonly used <a href="https://github.com/HumanSignal/label-studio" target="_blank">label-studio</a> or <a href="https://github.com/SkalskiP/make-sense" target="_blank">make-sense</a> . Label-studio is nice because you can export in YOLO format which is a nice speedup.</p>

<p><img src="/assets/od_bounding_boxes.png" alt="[od_bounding_boxes.png]" /></p>

<p>For the demo, we&rsquo;ll use Ultralytics&rsquo;s library as it makes working with YOLO models easy, but there are numerous others on huggingface. Let&rsquo;s download the YOLO 11 model, Ultralytic&rsquo;s most recent model. The model is in pytorch format, and we can see it includes 80 classes out of the gate.</p>
<pre><code class="language-python"><pre tabindex="0" class="ch-chroma"><code><span class="ch-line"><span class="ch-cl"><span class="ch-kn">from</span> <span class="ch-nn">ultralytics</span> <span class="ch-kn">import</span> <span class="ch-n">YOLO</span>
</span></span><span class="ch-line"><span class="ch-cl">
</span></span><span class="ch-line"><span class="ch-cl"><span class="ch-n">model</span> <span class="ch-o">=</span> <span class="ch-n">YOLO</span><span class="ch-p">(</span><span class="ch-sa"></span><span class="ch-s1">&#39;</span><span class="ch-s1">yolo11n.pt</span><span class="ch-s1">&#39;</span><span class="ch-p">)</span>
</span></span><span class="ch-line"><span class="ch-cl"><span class="ch-nb">print</span><span class="ch-p">(</span><span class="ch-sa">f</span><span class="ch-s2">&#34;</span><span class="ch-s2">Number of classes: </span><span class="ch-si">{</span><span class="ch-nb">len</span><span class="ch-p">(</span><span class="ch-n">model</span><span class="ch-o">.</span><span class="ch-n">names</span><span class="ch-p">)</span><span class="ch-si">}</span><span class="ch-s2">&#34;</span><span class="ch-p">)</span>
</span></span><span class="ch-line"><span class="ch-cl"><span class="ch-nb">print</span><span class="ch-p">(</span><span class="ch-sa">f</span><span class="ch-s2">&#34;</span><span class="ch-s2">Classes: </span><span class="ch-si">{</span><span class="ch-n">model</span><span class="ch-o">.</span><span class="ch-n">names</span><span class="ch-si">}</span><span class="ch-s2">&#34;</span><span class="ch-p">)</span>
</span></span></code></pre></code></pre>

<p>I am using the following standard hierarchy for storing the training data.</p>
<pre><code class="language-bash"><pre tabindex="0" class="ch-chroma"><code><span class="ch-line"><span class="ch-cl">datasets/
</span></span><span class="ch-line"><span class="ch-cl">└── stormtrooper/
</span></span><span class="ch-line"><span class="ch-cl">    ├── train/
</span></span><span class="ch-line"><span class="ch-cl">    │   ├── images/
</span></span><span class="ch-line"><span class="ch-cl">    │   └── labels/
</span></span><span class="ch-line"><span class="ch-cl">    └── val/
</span></span><span class="ch-line"><span class="ch-cl">        ├── images/
</span></span><span class="ch-line"><span class="ch-cl">        └── labels/
</span></span></code></pre></code></pre>

<p>Let&rsquo;s designate out a yaml file to set the parameters for the model.</p>
<pre><code class="language-yaml"><pre tabindex="0" class="ch-chroma"><code><span class="ch-line"><span class="ch-cl"><span class="ch-nt">path</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">stormtrooper</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">train</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">train</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">val</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">val</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">names</span><span class="ch-p">:</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-w">  </span><span class="ch-nt">0</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">stormtrooper</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">hsv_h</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">.015</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">hsv_s</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">.7</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">hsv_v</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">.4</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">translate</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">.3</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">scale</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">.5</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">shear</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">.01</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">flipud</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">.3</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">fliplr</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">.5</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">mixup</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">.5</span><span class="ch-w">
</span></span></span><span class="ch-line"><span class="ch-cl"><span class="ch-w"></span><span class="ch-nt">auto_augment</span><span class="ch-p">:</span><span class="ch-w"> </span><span class="ch-l">randaugment</span><span class="ch-w">
</span></span></span></code></pre></code></pre>

<p>Now for training the model! I&rsquo;m on a mac with a M-series chip but if you&rsquo;re working with a gpu you can use <code>device=cuda</code>.</p>
<pre><code class="language-shell"><pre tabindex="0" class="ch-chroma"><code><span class="ch-line"><span class="ch-cl">yolo train <span class="ch-nv">data</span><span class="ch-o">=</span>datasets/stormtrooper.yaml <span class="ch-nv">device</span><span class="ch-o">=</span>mps
</span></span></code></pre></code></pre>

<p>After training is complete, the &ldquo;runs/detect&rdquo; directory contains reporting on the success of the training. The best version of the model is in &ldquo;runs/detect/train/weights/best.pt&rdquo;.</p>

<p><img src="/assets/od_results.png" alt="Results" /></p>

<p>Let&rsquo;s print the classes for the best model and evaluate it.</p>
<pre><code class="language-python"><pre tabindex="0" class="ch-chroma"><code><span class="ch-line"><span class="ch-cl"><span class="ch-n">model</span> <span class="ch-o">=</span> <span class="ch-n">YOLO</span><span class="ch-p">(</span><span class="ch-sa"></span><span class="ch-s1">&#39;</span><span class="ch-s1">runs/detect/train/weights/best.pt</span><span class="ch-s1">&#39;</span><span class="ch-p">)</span>
</span></span><span class="ch-line"><span class="ch-cl"><span class="ch-nb">print</span><span class="ch-p">(</span><span class="ch-n">model</span><span class="ch-o">.</span><span class="ch-n">names</span><span class="ch-p">)</span>
</span></span><span class="ch-line"><span class="ch-cl"><span class="ch-n">model</span><span class="ch-o">.</span><span class="ch-n">val</span><span class="ch-p">(</span><span class="ch-p">)</span>
</span></span></code></pre></code></pre>

<p>mAP50 was <strong>0.995</strong> and mAP50-95 was <strong>0.851</strong>. This works for our use case. The scoring is done on a 0-1 scale, so 0.955 and 0.851 will both yield good results in the context of our training data. The difference between the mAP50 and mAP50-95 is the threshold of object overlaps. None of our training data had overlaps so it is expected that the mAP50 has a higher score.</p>

<p>The usefulness of the model is a combination of training data and real world use needs. All our data is trained on a single table, so if the model is only expected to detect Stormtroopers on this table, it&rsquo;ll perform great. However, if we want to detect objects in different settings and different lighting conditions, a more generalized and larger dataset would likely be required as we&rsquo;re probably overfitting.</p>

<p>Now let&rsquo;s do a few predictions and save the results!</p>
<pre><code class="language-python"><pre tabindex="0" class="ch-chroma"><code><span class="ch-line"><span class="ch-cl"><span class="ch-n">model</span><span class="ch-o">.</span><span class="ch-n">predict</span><span class="ch-p">(</span>
</span></span><span class="ch-line"><span class="ch-cl">	<span class="ch-sa"></span><span class="ch-s1">&#39;</span><span class="ch-s1">./datasets/stormtrooper/val/images/784e92f8-IMG_1908.jpeg</span><span class="ch-s1">&#39;</span><span class="ch-p">,</span>
</span></span><span class="ch-line"><span class="ch-cl">	<span class="ch-n">imgsz</span><span class="ch-o">=</span><span class="ch-mi">320</span><span class="ch-p">,</span>
</span></span><span class="ch-line"><span class="ch-cl">	<span class="ch-n">save</span><span class="ch-o">=</span><span class="ch-kc">True</span><span class="ch-p">)</span>
</span></span></code></pre></code></pre>

<p><img src="/assets/od_predict.png" alt="Prediction Image" /></p>

<p>In every frame were were successful in identifying the Lego Stormtroopers. The final trained model is 5.5mb and performant to handle high FPS use-cases. The benefit of YOLO and other newer models is this fine-tuned model could be executed on an edge device and do real-time object detection.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Object detection models have come a long way. They&rsquo;re now capable of efficiently detecting objects in high FPS environments. Our fine-tuned example was simple with limited training data but it still provided accurate detections.</p>

    </div>
</article>

    </main>
</body>
</html>



